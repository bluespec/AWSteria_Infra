= AWSteria_Infra
Rishiyur S. Nikhil, Bluespec, Inc. (c) 2021
:revnumber: v1.0
:revdate: 2021-08-16
:sectnums:
:toc:
:toclevels: 5
:toc: left
:toc-title: Contents
:description: Infrastructure for host+FPGA apps, and an example test app.
:keywords: AWS, F1, Shell, Instance AFI, AMI, DCP, Design Checkpoint, Custom Logic
:imagesdir: Figures
:data-uri:

// ================================================================
// SECTION
== AWSteria_Infra Overview

AWSteria_Infra is for apps that run jointly on a standard host
computer "`host side`" and an attached FPGA board ("`hardware side`").
The goal is for an app written against this API to be ported trivially
across a variety of supported platforms (host/FPGA pairings).

The initial set of platforms are (more likely in future):

* Simulation in Bluesim and Verilator simulation (infrastructure is in `Platform_Sim/`).

* Xilinx VCU118 FPGA board (infrastructure is in `Platform_VCU118/`).

* Amazon AWS F1 (infrastructure is in `Platform_AWSF1/`).

The infrastructure provides:

* A high-level API for the host side and hardware side to communicate.

* A high-level API for the hardware side to access FPGA DDR memory.

* Implementations for a number of platforms (host and hardware
    combinations) including, for some FPGAs, reclocking facilities for
    the hardware side to run at slower clock speeds if needed.

We also provide a small Test Application (in `TestApp/`) to test that
the infrastructure is working properly on each platform.  TestApp is
also useful during development of support for new platforms.

// ================================================================
// SECTION
== APIs (i.e., interface specs for apps)

The APIs are specified in files in the `Include_API/` directory.

The API is inspired by Xilinx XDMA facilities (at
https://github.com/Xilinx/dma_ip_drivers[]) and the "`shell`" in the
Amazon AWS F1 HDK/SDK (at https://github.com/aws/aws-fpga.git[]).
Implementations are provided on XDMA and on AWS F1.

// ----------------------------------------------------------------
// SUBSECTION
=== Hardware side API

AWSteria_Infra defines the top-level Verilog or Bluespec BSV interface
for your app hardware.  This interface has several ports:

* Two sets of clock-and-reset (a default and an extra).

* An AXI4 M interface (manager/master) through which the host communicates
    with your hardware (wide interface, high bandwidth, typically used for moving data).

* An AXI4-Lite M interface (manager/master) through which the host
    communicates with your hardware (32-bit wide, typically used for control/status).

* One to four An AXI4 S interface (subordinate/slave) through which
    your hardware connects to DDR memories on the FPGA board.

* A few miscellanous utility ports (evironment-ready input, DUT-halted output, 4ns counter input).

If you are coding directly in Verilog, use the following file as a
starting point: it is an "`empty`" module with the required module
name and port list; you can populate the interior with your
application-specific logic (including instantiating sub-modules, etc.)

----
    Include_API/mkAWSteria_HW_EMPTY.v
----

The port list looks like this, in summary:
----
    module mkAWSteria_HW (CLK,
                          RST_N,
                          CLK_b_CLK,
                          RST_N_b_RST_N,

                          ... AXI4 M interface ports for host communication ...
                          ... AXI4-Lite M interface ports for host communication ...
                          ... AXI4 S interface port(s) for DDR communication ...

                          m_env_ready_env_ready,
                          m_halted,
                          m_glcount_glcount);
----

Here, `CLK` and `RST_N` are the default clock and reset,
and `CLK_b_CLK` and `RST_N_b_RST_N` are the extra clock-reset pair
(your app can ignore the extra pair if they are not needed).

If you are coding in BSV, use the following files as a starting point:

----
    Include_API/AWSteria_HW_EMPTY.bsv
    Include_API/AWSteria_HW_IFC.bsv
----

The former defines an "`empty`" BSV module with the required module
name and interface.  The latter defines the required interface.  When
compiled with the Bluespec `bsc` compiler it will produce a Verilog
module with the required module name and port list.

The BSV module header looks like this:

----
    module mkAWSteria_HW #(Clock b_CLK, Reset b_RST_N)
       (AWSteria_HW_IFC #(AXI4_Slave_IFC #(16, 64, 512, 0),
                          AXI4_Lite_Slave_IFC #(32, 32, 0),
                          AXI4_Master_IFC #(16, 64, 512, 0)));
----

If you are coding in some other HDL or using HLS, you can either
arrange for it to compile your top-level module to look like:

----
    Include_API/mkAWSteria_HW_EMPTY.v
----

or manually instantiate your top-level module inside this empty module.

Of course, when targeting an FPGA platform (Amazon AWS F1, Xilinx
VCU118, ...)  your Verilog RTL should be acceptable to the synthesis
tool for that platform.

// ----------------------------------------------------------------
// SUBSECTION
=== Host side API

On the host side, AWSteria_Infra defines a C API through which your
host-side application communicates with the hardware via the AXI4 M
and AXI4-Lite M ports described above.

----
    Include_API/AWSteria_Host_lib.h
----

Briefly, it contains an intialization and an shutdown call, a pair of
read/write functions to communicate via the AXI4 M port, and a pair of
read/write functions to communicate via the AXI4-Lite M port.

Host side code can be written in any language environment.  To
communicate with the hardware side it should invoke the C host-side API.

`AWSteria_Infra` provides C code implementing the API for each
platform.

// ================================================================
// SECTION
== Platform: Simulation (Bluesim and Verilator)

The `Platform_Sim/` directory provides an implementation of
AWSteria_Infra for simulation.

* The host side and hardware side run as two processes on a standard computer.
* The hardware side runs in simulation, Bluesim or Verilator
    simulation (it can be ported easily to other Verilog simulators).
* The AWSteria_Infra host-hardware communication is emulated over TCP/IP.
* The AWSteria_Infra DDR memory interfaces are connected to memory models.

The "`Test Application`" and "`Porting your application`" sections
illustrate how to build and run an application on AWSteria_Infra in
simulation.

In general, you won't have to modify anything in this directory or
build anything in this directory; it just provides resources for your
application-build.

// ================================================================
// SECTION
== Platform: VCU118 board (from Xilinx)

The `Platform_VCU118/` directory provides an implementation of
AWSteria_Infra for a standard Debian/Ubuntu computer with a
Xilinx VCU118 FPGA board attached with a PCIe bus.

The "`Test Application`" and "`Porting your application`" sections
illustrate how to build and run an application on AWSteria_Infra on
VCU118.

In general, you won't have to modify anything in this directory or
build anything in this directory; it just provides resources for your
application-build.

// ----------------------------------------------------------------
// SUBSECTION
=== VCU118 Host side

`Host/AWSteria_Host_lib.c` implements the host-side API, invoking
various system calls to interact with the Xilinx XDMA driver, to
communicate with the FPGA.

`Host/Cmd_Line_Tests.mk` shows examples of using command-line
tools provided in the Xilinx XDMA driver repo to read and write
through the AXI4 and AXI4-Lite buses into the hardware side:
`dma_from_device`, 
`dma_to_device`, 
`reg_rw`.

The `dma_to_device` tool optionally takes data from a file, to be written to the FPGA.
`Host/gen_test_data.c` is a small program to generate such a file.

// ----------------------------------------------------------------
// SUBSECTION
=== VCU118 Hardware side

`HW/AWSteria_HW_reclocked/` is a Vivado Block Design project that was
used to create the "`reclocking layer`" for `AWSteria_HW_IFC.bsv` that
allows the app to run at slower clock speeds than the Garnet-supplied
250 MHz.  I.e., it creates a module which is "`shim`" that:

* Instantiates a app module (with the  `AWSteria_HW_IFC.bsv` interface), and

* The shim itself presents the same `AWSteria_HW_IFC.bsv` interface interface.

* Inside the shim, it:

  ** Instantiates a clock divider so that the inner module receives
     two sets of clock-and-reset, at 100 MHz and 50 MHz, respectively,

  ** Instantiates clock crossings between corresponding the outer and inner interfaces.

This allows the user's design (inner app module instance) to run at a slower clock.

In Vivado, the "Generate Block Deign" action creates and populates the
following directory:

----
    AWSteria_HW_reclocked/AWSteria_HW_reclocked.srcs/sources_1/bd
----

which is copied into `example_AWSteria_HW_reclocked/src/bd` (see below).

TODO: Instead of copying `.bd/` it should be possible to copy just a Tcl script that encodes the Block Design.

Unless you want to change the clock speed configurations, or change
the interfaces, this Block Design project step does not have to be
repeated.

`HW/example_AWSteria_HW/` and `HW/example_AWSteria_HW_reclocked/` are
template directories for Garnet, and are copied into the app's build
directories (see VCU118 flow for Test Application below).  The former
is meant for apps that can run at the full 250 MHz Garnet clock speed
(and so do not need the reclocking shim); the latter is meant for apps
that must run at slower clocks speeds and need the reclocking shim.

`HW/synchronizers.v` contains small RTL modules used by the reclocking
shim for reset synchronization, 1-bit clock-crossing synchronization,
and 64-bit clock-crossing synchronization.  These instantiate and
customize modules from the following IP in the Xilinx IP directories.

----
    /tools/Xilinx/Vivado/2019.1/data/ip/xpm/xpm_cdc/hdl/xpm_cdc.sv
----

// ================================================================
// SECTION
== Platform: Amazon AWS F1

The `Platform_AWSF1/` directory provides an implementation of
AWSteria_Infra for an Amazon AWS F1 instance (i.e., a server
in the cloud with an FPGA board attached with a PCIe bus).

The "`Test Application`" and "`Porting your application`" sections
illustrate how to build and run an application on AWSteria_Infra on
AWS F1.

In general, you won't have to modify anything in this directory or
build anything in this directory; it just provides resources for your
application-build.

// ----------------------------------------------------------------
// SUBSECTION
=== AWS F1 Host side

`Host/AWSteria_Host_lib.c` implements the host-side API, invoking
various functions in AWS' `aws-fpga` SDK libraries to communicate with
the FPGA.

// ----------------------------------------------------------------
// SUBSECTION
=== AWS F1 Hardware side

`HW/` contains some SystemVerilog files that are a wrapper around the
app RTL, and which plugs into the so-called "`shell`" in the AWS'
`aws-fpga` HDK.  The shell connects the host-communication AXI4 and
AXI4-Lite interfaces to the PCIe bus, and the DDR interfaces to DDRs
on the FPGA board.

// ================================================================
// SECTION
== Test Application

The `TestApp/` directory provides a small and simple test application.
When you create a new application, you could use this as a starting
template and modify it for purpose (see Section "`Porting your
application`" for more details).

// ----------------------------------------------------------------
// SUBSECTION
=== App source code and intended behavior

`TestApp/Host/main.c` is the host-side source code; it invokes the
host side C API `Include_API/AWSteria_Host_lib.h`.

`TestApp/HW/AWSteria_HW.bsv` is the hardware-side source code, filling
out the "`empty`" module provided in
`Include_API/AWSteria_HW_EMPTY.bsv`.

The hardware side is simple: it connects the host AXI4-Lite interface
to an AXI4-Lite-to-AXI4 adapter which, along with the host AXI4
interface connects to a 2x2 AXI4 crossbar switch which, in turn,
connects to two AXI4 DDR interfaces.

The host side simply writes random data to hardware-side DDRs, and
reads them back to verify the data.  Writes and reads are performed
over both the host AXI4 and AXI4 Lite interfaces, including writing
through one and reading through the other.  The AXI4 interface is also
exercised with large writes and reads, to exercise AXI4 burst
transfers.

// ----------------------------------------------------------------
// SUBSECTION
=== Building and running with Bluesim or Verilator simulation

* In `TestApp/Host/build_sim` do `make` to create the host-side executable `exe_Host_sim`.

* In `TestApp/HW/build_Bluesim` do `make all` to create the HW-side simulation executable `exe_HW_sim`.
+
or,
+
in `TestApp/HW/build_Verilator` do `make all` to create the HW-side simulation executable `exe_HW_sim`.

* Run the hardware side executable in one process (e.g., in one
    terminal window) It will await a TCP connection on a TCP port from
    the host side; it will then execute the hardware.

* Run the host side executable in another process (e.g., in another
    terminal window) It will connect using TCP to the hardware side
    and then interact with the hardware side, displaying messages
    about its actions (reading and writing to DDRs on the hardware
    side).

You will have to kill the HW-side process when done (e.g., using
`^C`).  You can restore each build directory to its pristine state
with `make full_clean`.

// ----------------------------------------------------------------
// SUBSECTION
=== Building and running on Xilinx VCU118

// ----------------
// SUBSUBSECTION
==== Prerequisites: Xilinx XDMA and XVSEC drivers, and Vivado tool suite

Please install Xilinx's XDMA and XVSEC drivers on your host Linux
machine, where your VCU118 is attached using PCIe.  The drivers can be
found at: https://github.com/Xilinx/dma_ip_drivers.git[].

The XVSEC installation will install the `xvssecctl` tool and driver,
which is used for "`partial reconfiguration`" of the FPGA with a
partial bitfile.  After intallation you'll see files like this
`/dev/xvsec*` on your Linux host, and the following executable tool:
`/usr/local/sbin/xvsecctl`.

The XDMA installation will install the `xdma` driver in your Linux
kernel.  After intallation you'll see files like this `/dev/xdma*` on
your Linux host.

You will also need to have installed Xilinx's Vivado tool suite, and
have a Vivado license that includes synthesis for the VCU118.

// ----------------
// SUBSUBSECTION
==== Prerequisite: Garnet

The Garnet repo (author: Jessica Clarke, Cambridge University, UK) can
be cloned from https://github.com/CTSRD-CHERI/garnet[].

Garnet provides PCIe and DDR infrastructure for VCU118, and a 250 MHz
clock and reset.  Please download Garnet and follow the instructions
there to build and run the provided simple example.

The Garnet flow installs two separate bitfiles on the VCU118, using
Xilinx's "`partial reconfiguration`" mechanism.  The first bitfile is
for a component called the "`shell`" and contains the fixed,
unchanging support for PCIe and DDR4s.  This component needs to be
loaded just once

The second bitfile, a "`partial bitfile`", contains the the logic for
the provided example, i.e., the application-specific logic.  This
component can be replaced with partial reconfiguration, as we change
or replace the example.

RTL for AWSteria_Infra plugs into the Garnet "`shell`" environment,
i.e., it is an alternative partial bitfile to the Garnet-provided
example.

// ----------------
// SUBSUBSECTION
==== Building the hardware side

Building the hardware side for VCU118 involves some steps locally in
the AWSteria_Infra repo, followed by a step in the "`Garnet`" repo.

An app in AWSteria_Infra can either run at Garnet's full speed (250
MHz), or it can run at a slower clock speed; AWSteria_Infra provides
the slower clock, and suitable clock-crossing logic.

We describe first the flow for a full speed app, and then the slight
variation for a slower speed app.

The following steps are performed in the AWSteria_Infra repo (the two
`make` commands combined into one):

* In `TestApp/HW/build_VCU118` do `make compile`. This will create
    a directory `RTL/` and populate it with Verilog RTL generated
    from the BSV source code by the Bluespec `bsc` compiler.

* In `TestApp/HW/build_VCU118` do `make for_garnet`.  This will
    create a directory `example_TestApp/` that is ready to run
    through the Garnet flow.

Copy the `example_TestApp/` directory into the top-level of the
Garnet repo; change to that directory, and `make`:

----
    ... copy example_TestApp directory to garnet repo ...
    $ cd garnet/example_TestApp
    $ make
----
Garnet will run Vivado on TestApp RTL, eventually producing a "`partial bitfile`":
----
    garnet/example_TestApp/build/AWSteria_pblock_partition_partial.bit
----
This takes about 1 hour on a 12-core, 1.1 GHz, Intel Core i7-10710U CPU.

To build TestApp to run at the slower clock speed (100 MHz), the steps are analogous:

* In `TestApp/HW/build_VCU118` do `make for_garnet_reclocked`.  This will
    create a directory `example_TestApp_reclocked/` that is ready to run
    through the Garnet flow.

Copy the `example_TestApp_reclocked/` directory into the top-level of the
Garnet repo; change to that directory, and `make`:

----
    ... copy example_TestApp_reclocked directory to garnet repo ...
    $ cd garnet/example_TestApp
    $ make
----
Garnet will run Vivado on TestApp RTL, eventually producing a "`partial bitfile`":
----
    garnet/example_TestApp_reclocked/build/AWSteria_pblock_partition_partial.bit
----

// ----------------
// SUBSUBSECTION
==== Installing the partial bitfile into the VCU118 FPGA

This uses the `xvsecctl` tool and `xvsec` driver (see Xilinx Prerequisites section earlier).

Example Makefile fragment to perform the parital bitfile reconfiguration:

----
BUS           = 0x07
DEVICE_NO     = 0x0
CAPABILITY_ID = 0x1
BITFILE       = garnet/example_TestApp/build/AWSteria_pblock_partition_partial.bit

reconfig:
        sudo xvsecctl -b $(BUS) -F $(DEVICE_NO) -c $(CAPABILITY_ID) -p $(BITFILE)
----

// ----------------
// SUBSUBSECTION
==== Building and running the host side

In `TestApp/Host/build_VCU118` do `make` to create the host-side
executable `exe_Host_VCU118`.

Then, run the executable.  It will interact with the hardware on the
FPGA.  The console output should be exactly the same as running in
simulation (described earlier).

// ----------------------------------------------------------------
// SUBSECTION
=== Building and running on Amazon AWS F1

// ----------------
// SUBSUBSECTION
==== Prerequisites: aws-fpga SDK and HDK, and Vivado

Please clone Amazon's aws-fpga SDK and HDK, which can be found at
https://github.com/aws/aws-fpga.git[], and initialize them as
described in its README (sourcing `hdk_setup.sh` and `sdk_setup.sh`).

If you are building on your own computer ("`own premises`"), you will
also need to have installed Xilinx's Vivado tool suite and have a
Vivado license for synthesis for the FPGA part that is on AWS F1
instances.

Alternatively, you can do your builds on the Amazon cloud using an
"`FPGA Developer`" AMI (Amazon Machine Instance), which comes with
aws-fpga and Vivado tools and licenses pre-installed.

// ----------------
// SUBSUBSECTION
==== Building the hardware side

The following steps are performed in the AWSteria_Infra repo (the two
`make` commands combined into one):

* In `TestApp/HW/build_AWSF1` do `make compile`. This will create
    a directory `RTL/` and populate it with Verilog RTL generated
    from the BSV source code by the Bluespec `bsc` compiler.

* In `TestApp/HW/build_AWSF1` do `make for_AWSF1`.  This will
    create a directory `example_TestApp/` that is ready to run
    through the aws-fpga HDK flow.


Run that RTL
through the standard Amazon AWS F1 HDK flow: build DCP (Design
Checkpoint), create AFI (Amazon F1 Instance).  

Copy the `example_TestApp/` directory into the aws-fpga repo.

----
    ... copy example_TestApp directory to garnet repo ...
    $ cd garnet/example_TestApp
    $ make
----
This will run Vivado on TestApp RTL, eventually producing a "`Design Checkpoint `" (DCP).
----
    ...
----
This takes about ... time on a 12-core, 1.1 GHz, Intel Core i7-10710U CPU.

// ----------------
// SUBSUBSECTION
==== From DCP to AFI


We have successfully built and run it with AWS' "`clock recipe A1`" (250 MHz).


// ----------------
// SUBSUBSECTION
==== Building and running the host side

In `TestApp/Host/build_AWSF1` do `make` to create the host-side
executable `exe_Host_AWSF1`.

* On an Amazon AWS F1 instance, use AWS' fpga management tools to load
    the AFI into the FPGA (your app's hardware side).

* On the Amazon AWS F1 instance, run the host side executable.  It
    will interact with the hardware on the FPGA.  The console output
    should be exactly the same as running in simulation (described
    earlier).

// ================================================================
// SECTION
== Porting your application to use AWSteria_Infra

The small `TestApp` example and its build-and-run flow provides a
template for coding, building and running your app.  The
`Include_API/` files provide "`empty`" Verilog and BSV modules for
convenience, which you can use as your starting point.

Create your own app directory as a sibling to `TestApp`, with the same
structure (you can omit any of these platform-directories that you
don't need):

----
    MyApp/
        Host/
            build_sim/
            build_VCU118/
            build_AWSF1/
        HW/
            build_Bluesim/
            build_Verilator/
            build_VCU118/
            build_AWSF1/
----

Create Makefiles in each directory, using those in the corresponding
directories in TestApp as a starting template.

Follow the build-and-run flows described for TestApp.

// ================================================================
// SECTION
== Possible Future Evolution

* Port AWSteria_Infra to more platforms (more FPGA boards).  Note the
    host-FPGA communication does not have to be over PCIe; it could
    run over other transports such as Ethernet, USB, JTAG, ... (albeit
    with slower performance).  Indeed `Platform_Sim` described above
    uses TCP/IP as a transport.

`TestApp` can be augmented easily for other uses:

* Measure AWSteria_Infra performance: latencies and bandwidths for host-FPGA
    communication, for DUT-Memory access, etc.

* "`Unload`" DDR after some DUT has run in AWSteria_Infra, e.g.,
    application performance counters stored in DDR (for platforms
    where DDR contents are preserved across bitfile reloads).
    This would be a minor change to host side C code.

* "`Preload`" DDR before some DUT has run in AWSteria_Infra, e.g., a
    section of DDR used by the DUT as a ROM, or as initialized memory
    (for platforms where DDR contents are preserved across bitfile
    reloads).
    This would be a minor change to host side C code.

// ================================================================
